# Rancher HA Notes

A rancher HA cluster is a dedicated kubernetes cluster whose only tasks are to run rancher and related work loads. Actual product clusters are provisioned and administrated from the ui/api of this dedicated rancher cluster.


Getting a rancher HA cluster running can be challenging. The steps are:
1. Create/provision nodes
2. Install kubernetes with RKE (a kubernetes installer)
3. Initialize helm and install rancher through it.

## 1. Create/provision nodes
Rancher requires specific versions of docker and is compatible with certain linux distro's.

*Cgroup errors*: cgroups disabled, see ticket for more.

## Setting up the nodes

The nodes can be provisioned with terraform. This can be run by setting the `main.tf` to the rancher module, then doing `terraform apply`. 

The nodes will provision but will take several extra minutes before we can run RKE over them. In the meantime, use `scripts/generate_rke_inv > ../ansible/inventory/rke.yml` to generate an RKE inventory. This is a python script that will template out the IP addresses of the nodes.

SSH into one of the nodes and check if `4.txt` is in `/`. If it isn't, the user_data script is still running and the ndoe is not ready yet.

## The userdata script

The script has 4 main parts. It is not reproduced here because it is subject to change.

1. We give each node an extra 200gb of space, this section sets up the logical volumes so they can be used.

2. In the next step we install docker, start it up, and add our user to the docker group.

3. `/var` only has about 2gb of space, and that quickly fills as docker downloads images. So we stop docker, relocate `/var/lib/docker` to `/storage/docker` and create a symlink so that any operations to `/var/lib/docker` actually happen in `/storage/docker`

4. We stop the firewalls so the hosts can communicate with eachother, then we restart docker to clear its iptable rules.

## RKE

Once our user-data script is done running, we can run RKE to set up the cluster.
Using `rke up --config inventory/rke.yml` will start the deploy process.

When it is done it will create a kubeconfig file called `kube_config_rancher-inv.yml`, we can check on the k8s cluster with `kubectl --kubeconfig /home/ant/dev/git/grayskull/kubernetes/terraform/kube_config_rancher-inv.yml get all -A`. We should have a working vanilla k8s cluster now.

## Set up Helm

refer here: https://rancher.com/docs/rancher/v2.x/en/installation/ha/helm-init/

## set up rancher

refer here: https://rancher.com/docs/rancher/v2.x/en/installation/ha/helm-rancher/
We need to generate our own certs, otherwise the ingress will default to `rancher.my.org` so anything at `gsp.test` won't work. 

Include your own certs by following directions here: https://rancher.com/docs/rancher/v2.x/en/installation/ha/helm-rancher/tls-secrets/

Once you have certs, You can also simply run the rancher.yml playbook to automate the setup. Log in to the UI and set the rancher url to be the private DNS (like `ip-172-31-13-11.us-gov-west-1.compute.internal` of the node.

## Run ansible on it 

You can use ansible for deploying rancher using `rancher.yml` playbook. 

This step isn't nessecary unless you want to run some portion of the grayskull deploy on it.
In that case you can use `scripts/generate_rancher_ansible_inv > ../ansible/inventory/rancher.ini`
and `scripts/generate_ssh_config > /tmp/ssh_config`

## Make rancher the owner of the grayskull cluster

The terraform playbook actually provisions 9 nodes. 3 for rancher HA cluster, and 6 for grayskull.

You must use rancher's UI to set up the grayskull cluster (todo: This might be automatable using rancher cli or api). This is so rancher is the one managing the cluster.

From the global page, hit **add cluster**, choose **From my own existing nodes**.
Name it and set network provider to Calico.

Under **Advanced cluster options**, set **Nginx ingress** to *disabled*. Grayskull provides an ingress for itself.

Change **Docker Root Directory** to `/storage/docker`.

On the next page you will be given a command to run on each node to join it to the cluster.
For Masters, select **etcd** and **control plane** before using the command.
For Workers, select only **worker**

The nodes will fail to join because they can't reach the internal IP of the rancher cluster. They will be redirected by the NGINX ingress which is operating on the same port. To get around this go to **Clusters**->**local**->**system**->**load balancing**. **Add an ingress** called internal with settings **Specify a hostname to use**, then specify the Private DNS from above. Set the **path** to `/ping`, **target** to `rancher` in namespace cattle-system, and the **port** to 80

